{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief on implementing the backpropogation algorithm for a 2 layer Neural Network\n",
    "\n",
    "#### Problem\n",
    "Binary classification. We generate synthetic dataset using the sklearn library function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 100 data points, each having 2 features/dimensions belonging to two classes as shown in the visualization.\n",
    "\n",
    "Task is to build a 2 layer NN to classify a given point as belonging to either of these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as toy_data \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x,y= toy_data.make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=2,random_state=191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes/Labels in the data: [0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes/Labels in the data:\",np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f69cde376a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(x[:,0],x[:,1],c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall build a 2 layer neural network having 4 neurons in the hidden layer and as this is a binary classification task a single neuron in the output layer will suffice. All neurons have sigmoid activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2 layer Network\n",
    "    ![2layerNN](2_NN.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a random set of weights and biases with [Xavier initialization](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79). As the sigmoid gives a value between 0 and 1 we fix a threshold at 0.5 to classify the sample as belonging to class 1 if it's greater than 0.5 else class 0 . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experiment with cross-entropy loss and mean squared error as the cost function to optimize using vanilla <b>Gradient Descent </b> to update the weights and biases of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "During the forward pass we construct intermediate gate 'Z' at the input of each neuron which will allow us to easily compute the local gradients and chain the errors from the output back to input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2 layer Network with intermediate gates 'Z'\n",
    "![2layerNN_intermediate gate](2l_nn_intermediate_gate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'b' is the bias associated with the neuron, every neuron has it's own bias, for representation purposes only 2 bias  and 3 weights are shown. We will arrive at the expression for gradients for one weight and bias and then genaralize them for the rest of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight matrix\n",
    "\n",
    "For the network under consideration, each of the 4 neuron in the hidden layer has 2 edges connected to it, so there are 4x2 weights at the input layer and 1x4 weights in the output layer . \n",
    "\n",
    "At a layer, we arrange the weight matrix such that a row holds all the weights of the edges connected to a particular neuron. Thus, number of rows of the matrix is equal to the number of neurons in a layer.\n",
    "\n",
    "#### Layer 1\n",
    "Eg:The first row of the weight matrix for layer 1 will have weights w11 and w12 where wij indicates the weights going to neuron 'i' from 'j'.\n",
    "\n",
    "$$\n",
    "W_{l1} = \n",
    "\\begin{bmatrix}\n",
    "w11 & w12 \\\\\n",
    "w21 & w22  \\\\\n",
    "w31 & w32  \\\\\n",
    "w41 & w42  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are 4 neurons in the hidden layer & we have 4 rows in the weight matrix. \n",
    "\n",
    "'Z' is the input gate to a neuron which is the weighted sum of inputs.\n",
    "\n",
    "Eg: $ Z1 = w11*x1 + w12*x2 + b1 \\cdots (A1) $\n",
    "\n",
    "Writing it in a vector form, let $Z$ = [Z1 Z2  Z3  Z4] and $b$ =[b1 b2 b3 b4] be the vector of inputs and bias respectively to layer 1 so  \n",
    "$Z$ = $W_{l1}x_i + b$ for each input sample $x_i$ \n",
    "\n",
    "$ sigma(Z1) $ is the output of the first activation '1' \n",
    "\n",
    "In vectorized form $ sigma(Z) $  is vector of activations applied to Z.\n",
    "\n",
    "#### Layer 2\n",
    "$$ \n",
    "W_{l2} =\n",
    "\\begin{bmatrix}\n",
    "w1 \\\n",
    "w2 \\\n",
    "w3 \\\n",
    "w4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "###### Note: We are using the same convention as above, all weights connecting to a neuron form a row\n",
    "\n",
    "Therefore\n",
    "$ Z_f = w1*sigma(Z1)+w2*sigma(Z2)+w3*sigma(Z3)+w4*sigma(Z4) + b $ <b> ... (1)</b>\n",
    "\n",
    "In matrix form,\n",
    "$$ Z_f = W_{l2}sigma(Z) + b$$ \n",
    "Output of the network\n",
    "$$ F = sigma(Z_f) $$\n",
    "\n",
    "### Summary\n",
    "\n",
    "Forward pass is about computing the matrix product of weights and input/neuron output and adding the bias untill we reach the final layer which is the output of the neural network.\n",
    "\n",
    "An alternate way to see the matrix product of weights and neurons is as the sum of the product of each neuron and the weights going out from them to the next layer.\n",
    "\n",
    "In layer 1 consider the sum of the product of input $x1$ , $x2$ and the corresponding the weights going out from them to layer2 + the bias for each neuron\n",
    "\n",
    "$$\n",
    "x1\n",
    "\\begin{bmatrix}\n",
    "w11 \\\\\n",
    "w21  \\\\\n",
    "w31  \\\\\n",
    "w41  \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "x2\n",
    "\\begin{bmatrix}\n",
    "w12 \\\\\n",
    "w22 \\\\\n",
    "w32 \\\\\n",
    "w42 \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b1 \\\\\n",
    "b2 \\\\\n",
    "b3 \\\\\n",
    "b4 \\\\\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "w11 & w12 \\\\\n",
    "w21 & w22  \\\\\n",
    "w31 & w32  \\\\\n",
    "w41 & w42  \\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "x1\\\\\n",
    "x2\n",
    "\\end{bmatrix} + b_{l1} = W_{l1}x_i + b_{l1}= Z\n",
    "$$\n",
    "\n",
    "\n",
    "$sigma(Z) = \\begin{bmatrix} sigma(Z1) \\\\ sigma(Z2) \\\\ sigma(Z3) \\\\ sigma(Z4)\\end{bmatrix} $ is the output of layer1 \n",
    "\n",
    "Similary for layer2 consider the sum of the product of the output of the neurons in the layer1 and the corresponding weights + bias which is the same form as in $(1)$ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backward pass we start from the last layer.\n",
    "\n",
    "With the current set of weights and bias we find the ouput of the network for a given sample. Next, we need a metric to measure the performance of the network, how close/far we are from the desired response based on which we change the weights & bias. For this let's define a error/cost function which we wish to reduce by tuning the weights and bias.\n",
    "\n",
    "Let's choose <b>squared error</b> as the cost function to optimize. As the network output $F$ gets closer to the known labels $y$ cost reduces and the cost increases if the predicted value diverges from the true labels.\n",
    "\n",
    "Cost $C = \\frac{(y-F)^2}{2} \\cdots (2) $ \n",
    "\n",
    "If you expand $F$ we can see that the cost is a function of the weights and bias of the network. Label $y$ and the input are not variables.\n",
    "\n",
    "<sub> Note: $\\frac{1}{2}$ is used so that on differentiating 2's cancel out. </sub>\n",
    "\n",
    "  <sub> Later we will see how to use the cross entropy loss </sub>\n",
    "\n",
    "#### How do we change the weights and biases?\n",
    "\n",
    "Our objective here is to reduce the cost/error function by changing the weights and bias. \n",
    "\n",
    "A slight digress to see two approaches to achieve our objective using a toy example of a parabola.\n",
    "\n",
    "Consider a function $f = (x+1)^2 + 1 $, a parabola for which we want to know the value of $x$ at which it attains it's minimum value, similar to our objective of finding those weights & bias where the cost attains it's minimum value, only that our toy example has a single parameter $x$. \n",
    "\n",
    "To find the minimum of a function analytically we can find the slope of the function and check where it is zero which gives us the min/max points. For the $f$ defined above, setting the derivate -which gives us the slope at a point- to zero $\\frac{df}{dx} = 2(x+1)$  gives us $x=-1$ . Based on the second derivate test we can conclude that this is a minimum point.  In our problem too we need to find a set of weights & bias for which our cost function attains it's minimum value, but our network has 17 parameters & we can see that the above procedure is cumbersome to find the minimum analytically. For this we have to explictly write out the equation interms all 17 parameters and take the partial derivatives etc.. \n",
    "\n",
    "Instead, we use gradient descent where we start at a random point as determined by the initial weights and bias. We find the Gradient-partial derivative w.r.t each direction- of the cost function i.e w.r.t each weight and bias(we shall see how to do that in the next section) and move a small distance in the direction opposite to the one indicated by the Gradient by changing the weights and bias. Then, using this new set of weights & bias we find the network output and repeat the above process. \n",
    "\n",
    "After computing the gradient w.r.t each weight and the bias we update them as follows\n",
    "<h5>\n",
    "w  = w - learning_rate*gradient_Cost_weight\n",
    "\n",
    "b = b - learning_rate*gradient_Cost_bias\n",
    "</h5>\n",
    "\n",
    "In practice, updates are not normally done after we run through each sample but in batches, where we accumulate the gradient over the samples in the batch and update the weights and bias once at the end of the batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eg: Consider the above function $f=(x+1)^2 + 1$. Suppose we wish to find the minimum using the gradient descent approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value -0.99999 Iteration 17\n"
     ]
    }
   ],
   "source": [
    "start_point = 12.237 # Some random point\n",
    "gradient=lambda x:2*(x+1) # this is the slope df/dx\n",
    "\n",
    "def gd(max_steps=30,learning_rate=0.1,x=start_point,threshold=1e-4):\n",
    "    \n",
    "    diff_grad = 1 # difference between gradient values in the successive iterations\n",
    "    prev_grad = 1 \n",
    "    steps=1\n",
    "    \n",
    "    # We stop iterating when either the difference between successive gradient values \n",
    "    # is less than a threshold or max_iteration steps is reached\n",
    "    \n",
    "    # As we get close to the min/max points where the gradient is zero, diff in the \n",
    "    # successive values of the gradient computed will be very small\n",
    "    \n",
    "    while ((diff_grad>threshold) and (steps<max_steps)): \n",
    "        \n",
    "        # Calculate the gradient at each step\n",
    "        grad = gradient(x)\n",
    "        # Move in the direction opposite to the gradient by a small distance as \n",
    "        # determined by the learning rate\n",
    "        \n",
    "        x = x - learning_rate * grad\n",
    "        \n",
    "        diff_grad = abs(prev_grad - grad)\n",
    "        prev_grad = grad\n",
    "        steps+=1\n",
    "        \n",
    "    return x,steps\n",
    "\n",
    "val,step = gd(50,0.3)\n",
    "print(\"Value %.5f Iteration %d\"%(val,step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, analytically we found out that the minimum val of $x$ is -1. We can experiment with different learning rates and thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Why does taking the gradient work ?\n",
    "\n",
    "Gradient gives us the direction of the steepest ascent, i.e direction in which the function increases the most if we change the parameters accordingly. But we want to reduce the cost so we go in the opposite direction of the Gradient. \n",
    "\n",
    "If a function $f(x,y,z)$ has 3 parameters, the gradient is given by $ \\nabla(f) = [\\frac{\\partial f}{\\partial x} \\frac{\\partial f}{\\partial y} \\frac{\\partial f}{\\partial z}] $\n",
    "\n",
    "<sub> Note: It can be mathematically proven that the direction given by the gradient is the one in which function increases the most. </sub>\n",
    "\n",
    "In 1-D we know that the slope of the function at a point is given by the derivative of the function evaluated at that point.<blockquote>\"The gradient is a generalization of slope for functions that donâ€™t take a single number but a vector of numbers\"</blockquote> By evaluating the gradient we are calculating the slope of the function in each dimension and we move in the direction opposite to the one indicated by the gradient for the reasons state above by a small distance as determined by the learning rate.\n",
    "\n",
    "<blockquote>\"The derivative on each variable tells you the sensitivity of the whole expression on its value\"</blockquote> Eg: If the slope of the function at a point is 'l' and we perturb the point by a infinitesimally small value h then the overall effect on network is to change the ouput by approximately 'lh' . \n",
    "\n",
    "Let's take $y=x^2$ , slope at any point is $2x$ . At $x=5$ , $y=25$ and slope is 10.\n",
    "Suppose we change x by h=0.001(Why should 'h' be small, derivate is defined only in an infinitesimally small region about a point, it is in the limit as $h->0$ so we consider only small changes) \n",
    "If we change x by h, change in the ouput is given by approx 10h = 0.01.\n",
    "\n",
    "Verify: (5.001)**2 = 25.010001... so the change is 0.01 . \n",
    "\n",
    "\n",
    "\n",
    "Coming back to the network...\n",
    "\n",
    "\n",
    "### Layer 2 weights and bias \n",
    "\n",
    "Let's find the gradient of the cost w.r.t weight in layer2 $W1$ as shown in the figure 2 above which is given by \n",
    "\n",
    "$$ \\begin{equation}\n",
    "  \\frac{\\partial C}{\\partial W_1} \\cdots (A) \n",
    "  \\end{equation}$$\n",
    "\n",
    "In computing this gradients we will make use of the intermediate gates, $Z$ . \n",
    "For the network under consideration we can calculate the gradient using chain rule as follows\n",
    "\n",
    "$$ \\begin{equation}\n",
    "   \\frac{\\partial C}{\\partial W_1} = \\frac{\\partial C}{\\partial Z_f} \\frac{\\partial Z_f}{\\partial W_1} \\end{equation}$$ \n",
    "   Where $Z_f$ is a function of $W_1$ and $C$ is a function of $Z_f$.\n",
    "   \n",
    "   From the equation (2) above \n",
    "   $C = \\frac{(y-F)^2}{2} $\n",
    "   \n",
    "   Where $F = sigma(Z_f)$\n",
    "   \n",
    "   Hence, \n",
    "      \n",
    "   \n",
    "   $$ \\begin{equation}\n",
    "      \\frac{\\partial C}{\\partial Z_f} = \\frac{\\partial C}{\\partial F} \\frac{\\partial F}{\\partial Z_f} \\end{equation} $$  \n",
    "      \n",
    "   $$ \\begin{equation}\n",
    "      \\frac{\\partial C}{\\partial F} = (F-y) \\cdots (L)\n",
    "      \\\\\n",
    "      \\frac{\\partial F}{\\partial Z_f} = sigma(Z_f)(1-sigma(Z_f)) = sigma^\\prime(Z_f)\n",
    "      \\end{equation}     \n",
    "   $$\n",
    "   \n",
    "   Thus,  $$ \\begin{equation}\n",
    "      \\frac{\\partial C}{\\partial Z_f} = (F-y) sigma^\\prime(Z_f) \\end{equation}  $$       \n",
    "   \n",
    "   \n",
    "   We'll use this value later to find the gradients of the cost w.r.t weights in layer 1, let's call this delta\n",
    "   \n",
    "   $$ \\textbf{delta} = \\frac{\\partial C}{\\partial Z_f} = (F-y) sigma^\\prime(Z_f) \\cdots (3) $$\n",
    "   \n",
    "From equation (1) above\n",
    "$$ \n",
    "    \\frac{\\partial Z_f}{\\partial W_1} = sigma(Z_1) \n",
    "    \\cdots (4) $$  \n",
    "    \n",
    "      \n",
    "  Using (3) & (4) above we get $$ \\frac{\\partial C}{\\partial W_1} =  \\frac{\\partial C}{\\partial Z_f}sigma(Z_1) = \\textbf{delta} \\cdot sigma(Z_1) \\cdots (W_{l2}) $$ as we desired to find out in $(A)$ \n",
    "  Same steps can be used to find the gradient w.r.t other weights in the second layer.\n",
    "  \n",
    "  \n",
    "  General expression for the gradient of weights in layer2 is $$ \\frac{\\partial C}{\\partial W_i} = \\textbf{delta} \\cdot sigma(Z_i) ; i:1->4 $$\n",
    "  \n",
    "  Similarly, gradient w.r.t bias 'b' is given by\n",
    "  \n",
    "  $$ \\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial Z_f} \\frac{\\partial Z_f}{\\partial b} $$ \n",
    "   Using (1) and (3)\n",
    "   $$ \\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial Z_f} $$\n",
    "   \n",
    "   \n",
    "### Layer 1 weights and bias \n",
    "\n",
    "   Let's find the gradient of the cost w.r.t weights in the layer 1. \n",
    "   Consider the weight $w11$.\n",
    "   \n",
    "   Gradient of the cost w.r.t weight $w11$ is given by \n",
    "   $\\frac{\\partial C}{\\partial w11}$\n",
    "   \n",
    "   In order to compute this we shall once again make use of the chain rule. We'll chain the gradients computed in layer2 with the local gradients in the layer1 .\n",
    "   \n",
    "   #### Rule\n",
    "   Find the intermediate gate -in this case Z1- derive the local gradient of the intermediate gate w.r.t the weight under consideration, find the gradient of the cost w.r.t the local gate by chaining with the gradients of the layer ahead the current one, in this case layer 2.\n",
    "   \n",
    "   We can compute    $$\\frac{\\partial C}{\\partial w11} = \\frac{\\partial C}{\\partial Z1}\\frac{\\partial Z1}{\\partial w11} \\cdots (5) $$\n",
    "    local gradient $$ \\frac{\\partial Z1}{\\partial w11} = x1 \\cdots (6) $$ is easy to compute using  $ Z1 = w11*x1 + w12*x2 + b1 \\cdots  $ as defined in (A1) in the forward pass section . \n",
    "    \n",
    "Now, by chain rule once again $$\\frac{\\partial C}{\\partial Z1} = \\frac{\\partial C}{\\partial Z_f}\\frac{\\partial Z_f}{\\partial Z1}$$ \n",
    "\n",
    "   Where  $Z_f$ and $Z1$ are one of the parameters of the function of $C$ and $Z_f$ respectively from (1) .\n",
    "\n",
    "Let's call $\\frac{\\partial C}{\\partial Z1}$ as $\\textbf{delta_z1}$ just as we did earlier.\n",
    "\n",
    "\n",
    "By (3) above we know $ \\frac{\\partial C}{\\partial Z_f}  $ which is $\\textbf{delta}$, Now all that's left is to compute is $ \\frac{\\partial Z_f}{\\partial Z1}$\n",
    "\n",
    "This is also easily computed from (1) above \n",
    "\n",
    "$$ \\frac{\\partial Z_f}{\\partial Z1} = w1sigma^\\prime(Z1) \\cdots (7) $$\n",
    "Thus, $$ \\textbf{delta_z1} = \\textbf{delta} \\cdot w1sigma^\\prime(Z1) \\cdots (8)$$\n",
    "\n",
    "Thus, from (6) (7) and (8) we get$$\\frac{\\partial C}{\\partial w11} = \\textbf{delta_z1} \\cdot x1 \\cdots (w_{l1})$$ \n",
    "\n",
    "Here, in layer 1 gradient of the weight is the product of local delta in layer 1 and the input data sample. \n",
    "\n",
    "For layer 2, gradient of the weight is the product of the local delta in layer 2 and the output of the activation of layer1 as seen in (4) above.\n",
    "\n",
    "Note how $\\textbf{delta}$ terms were constructed and chained with the earlier computed values as seen in (8)\n",
    "\n",
    "Gradient w.r.t bias is simple, as we did earlier, compute the local gradient and chain it with gradients flowing from the layer ahead of the current one.\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial b1} = \\textbf{delta_z1}\\frac{\\partial Z1}{\\partial b1}\n",
    "\\\\\n",
    "                                  = \\textbf{delta_z1}$$\n",
    "\n",
    "\n",
    "\n",
    "### Summary \n",
    "\n",
    "In the backward pass starting from the last layer compute the gradient of the cost function w.r.t the intermediate gate $Z_f as \\frac{\\partial C}{\\partial Z_f}$ and the local gradient $\\frac{\\partial Z_f}{\\partial W_i}$ . Product of (3) and (4) gives the gradient of the cost w.r.t  weight in layer 2 , similarly for bias.\n",
    "\n",
    "For weights in layer 1, we follow a similar procedure where compute the gradient of the cost w.r.t intermediate gate $ \\frac{\\partial C}{\\partial Z_i}$ and the local gradient $\\frac{\\partial Z_i}{\\partial W_{ij}}$ using (6) and (8) whose general expression is as follows\n",
    "   \n",
    "   * cost gradient w.r.t intermediate gate: $ \\textbf{delta_zi} = \\frac{\\partial C}{\\partial Z_i} = \\textbf{delta} * W_i * sigma^\\prime(Z_i) $\n",
    "   \n",
    "   * local gradient : $ \\frac{\\partial Zi}{\\partial w_{ij}} = x_j  $\n",
    "\n",
    "##### Expressing the operations in matrix form\n",
    "\n",
    "Layer 2/Output layer  weights\n",
    "\n",
    "Weight gradient : Product of $\\textbf{delta}$ and the vector output of layer 1 = \n",
    "                  $\\textbf{delta} \\cdot sigma(Z_{l1}) $\n",
    "\n",
    "For other layers \n",
    "\n",
    "1. Find the $\\textbf{delta's}$ associated with current layer using the sum of the product of $\\textbf{delta}$ of the layer ahead of the current one and the weights feeding into those corresponding intermediate gate.\n",
    "\n",
    "Layer 1 weights\n",
    "\n",
    "$\\textbf{delta_l1}  = \\textbf{delta} \\begin{bmatrix} W1 & W2 & W3 & W4 \\end{bmatrix} = \\textbf{delta} (\\cdot) W_{Transpose} $ as $W$ is a column vector , now $\\textbf{delta_l1}$ is a row vector.\n",
    "\n",
    "2. To find the gradients, take a matrix product of the $\\textbf{delta}$ computed in the step 1 with the inputs of the layer 1\n",
    "\n",
    " grad = $\\textbf{delta_l1}_{Transpose}  \\begin{bmatrix} x1 & x2 \\end{bmatrix} $ \n",
    " \n",
    " When we constructed the weight matrix earlier each row contained the weights connected to the neuron in layer ahead of the current one. Now, in step2 if we try to the match the weights with the edge connecting the input with neuron in layer 1 we see that the order of the gradients for the corresponding weights is maintained. \n",
    "\n",
    "For bias update we saw that we only need the $\\textbf{delta}$ associated with the intermediate gate.\n",
    "\n",
    "\n",
    "### Note:\n",
    "After computing the loss for each sample, we accumulate the gradient and do a weight update at the end of the epoch ( As the data set is small we shall use the full data to define the batch ) \n",
    "\n",
    "Without this approach of constructing intermediate gates and chaining the errors from the previous layers it becomes a humongous task to derive the gradients of the cost w.r.t each weight and bias \n",
    "\n",
    "Eg: Let's write the cost by seeing where weight $w11$ fits in using (1) and (A1) above ,\n",
    "    $$ C = (y - sigma(Z_f)) = (y-sigma( w1*sigma(Z1)+w2*sigma(Z2)+w3*sigma(Z3)+w4*sigma(Z4) + b)) =  (y-sigma( w1*sigma(w11*x1 + w12*x2 + b1)+w2*sigma(Z2)+w3*sigma(Z3)+w4*sigma(Z4) + b))$$ \n",
    "    \n",
    "We are dealing with a simple 2 layer network and the expression seems overwhelming to derive the gradient w.r.t w11 and for all the other weights and bias. Instead, in the backpropgation algorithm  we define intermediate gates, find the local gradients and chain them to get the requried result.\n",
    "\n",
    "We used mean squared error earlier, by the changing the loss function to cross entropy we only have to modify the loss in the final layer\n",
    "\n",
    "### Binary cross entropy loss \n",
    "\n",
    "Cross entropy loss is defined by \n",
    "$ C = -(ylog(F) + (1-y)log(1-F)) $ where $F = sigmoid(Z_f)$ It is easy to verify that if the prediction $F$ closely agrees with the actual label y(which is either 0 or 1) penalty is low else it is higher. \n",
    "Eg: If we expected label to be 1 but we got 0 penalty is high from $ylog(F)$ term\n",
    "\n",
    "From the above discussion we only have to calcuate the new value for delta at the layer 2 which is $\\frac {\\partial C}{\\partial Z_f} $ to see the performance of the network using the cross entropy loss function \n",
    "$$\\frac {\\partial C}{\\partial Z_f} = \\frac{\\partial C}{\\partial F} \\frac{\\partial F}{\\partial Z_f}$$ \n",
    "\n",
    "From the loss function defined above it can be verified that ,\n",
    "$$ \\frac {\\partial C}{\\partial F} = -\\frac{(F-y)}{(F)(1-F)}$$ and $$ \\frac{\\partial F}{\\partial Z_f}= F(1-F) $$ \n",
    "Thus, $$ \\frac {\\partial C}{\\partial Z_f} = (F-y) $$\n",
    "\n",
    "We only need to replace the $\\textbf{delta}$ arrived at in (3) which is $ (F-y) sigma^\\prime(Z_f)$ for mean squared error loss  with $(F-y)$  for cross entropy loss and the network is all set to go!\n",
    "\n",
    "Let's code it up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References : [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html),  [CS231n](https://cs231n.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the activation and it's derivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def dsigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-vectorized implementation\n",
    "\n",
    "Loss function : Squared error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    z = np.matmul(w1,x.T)+b1.reshape(-1,1)\n",
    "    l1_activation = sigmoid(z)\n",
    "    f = np.matmul(w2,l1_activation)\n",
    "    l2_activation = sigmoid(f)\n",
    "    return l2_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_acc: 48.48\n",
      "epoch: 100 train_acc: 86.87\n",
      "epoch: 200 train_acc: 88.89\n",
      "epoch: 300 train_acc: 88.89\n"
     ]
    }
   ],
   "source": [
    "layer=[2,4,1]\n",
    "\n",
    "\n",
    "#Xavier initialization\n",
    "\n",
    "#Layer 1 weights and bias\n",
    "w1=np.random.uniform(-1,1,size=(layer[1],layer[0]))*np.sqrt(6/(layer[0]+layer[1]))\n",
    "b1=np.zeros(shape=(layer[1]))\n",
    "\n",
    "#Layer 2 weights and bias\n",
    "w2=np.random.uniform(-1,1,size=(layer[2],layer[1]))*np.sqrt(6/(layer[1]+layer[2]))\n",
    "b2=np.zeros(shape=(layer[2]))\n",
    "\n",
    "lr = 1 # learning rate\n",
    "\n",
    "# to reserve some samples for testing\n",
    "train_idx = -1\n",
    "\n",
    "N = len(x[:train_idx])\n",
    "\n",
    "for epochs in range(301):\n",
    "     \n",
    "    # Accumulate the gradients for weights and bias after computing the loss for each sample\n",
    "    # and then update the weights & biases at the end\n",
    "    \n",
    "    w1_grad=np.zeros_like(w1)\n",
    "    w2_grad=np.zeros_like(w2)\n",
    "    b1_grad=np.zeros_like(b1)\n",
    "    b2_grad=np.zeros_like(b2)\n",
    "    \n",
    "    for idx in range(len(x[:train_idx])):\n",
    "        \n",
    "    ## Forward pass\n",
    "\n",
    "        # Iterate through each sample\n",
    "        # layer1\n",
    "        \n",
    "        # As defined above in the forward pass section, vector of input z is the matrix multiplication\n",
    "        # of weights and input, then we add a bias\n",
    "        \n",
    "        z = np.matmul(w1,x[idx])+b1\n",
    "        l1_activation = sigmoid(z)\n",
    "        l1_activation_gradient = dsigmoid(z)\n",
    "        \n",
    "        #layer2\n",
    "        f = np.matmul(w2,l1_activation)+b2\n",
    "        l2_activation = sigmoid(f)\n",
    "        l2_activation_gradient = dsigmoid(f)\n",
    "\n",
    "        # l2_activation is the output of the network\n",
    "        # In backward pass we first compute the loss/cost function and then derive the \n",
    "        # gradients w.r.t weights and biases\n",
    "    \n",
    "    ## Backward pass\n",
    "  \n",
    "        loss = (l2_activation - y[idx]) # As defined in (L) above in the backward pass section\n",
    "        \n",
    "        #layer2\n",
    "        delta = loss*l2_activation_gradient # As defined in (3) above in the backward pass section\n",
    "        w2_grad+=l1_activation*delta # As defined in (Wl2) above in the backward pass section\n",
    "        b2_grad+=delta\n",
    "        #layer1\n",
    "        \n",
    "        # As defined in (8) above in the backward pass section\n",
    "        loss = np.matmul(w2.T,delta)\n",
    "        delta = (loss*l1_activation_gradient) \n",
    "        \n",
    "        # As defined in (wl1) above in the backward pass section\n",
    "        w1_grad+= np.matmul(delta.reshape(-1,1),x[idx].reshape(1,-1))\n",
    "        b1_grad+= delta\n",
    "        \n",
    "    if(epochs%100==0):        \n",
    "        ypred_train = pred(x[:train_idx])\n",
    "        ypred_train = np.where(ypred_train>0.5,1.0,0).flatten()\n",
    "        print(\"epoch:\",epochs,\n",
    "              \"train_acc:\",np.round(accuracy_score(ypred_train,y[:train_idx])*100,2))      \n",
    "    \n",
    "    #update weights\n",
    "    w1 -= lr*w1_grad/N\n",
    "    w2 -= lr*w2_grad/N\n",
    "    b1 -= lr*b1_grad/N\n",
    "    b2 -= lr*b2_grad/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us vectorize the operation where we compute the gradients all at once for each sample and the update the weights and biases. We used two loops earlier, one for epochs and another to loop over the training sample. In vectorized implementation as our batch size is the same as the dataset we use only one loop for epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the vectorized implementation we arrange data such that each column in the input data matrix represents sample \n",
    "\n",
    "$$ data = \\begin{bmatrix}\n",
    "          | &  | \\cdots |\\\\\n",
    "          x1 & x2 \\cdots xn\\\\\n",
    "          | & | \\cdots |\n",
    "          \\end{bmatrix}$$\n",
    "          \n",
    "Our weight matrix structure remains the same\n",
    "\n",
    "Accumulation of the gradients to update the weights automatically happens by the virtue of matrix multiplication of the delta's and activation/input depending of the layer, in the non-vectorized implementation we manually added gradients after computing the loss for each sample. We only have sum the gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Vectorized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_acc: 51.0\n",
      "epoch: 10  train_acc: 67.0\n",
      "epoch: 20  train_acc: 81.0\n",
      "epoch: 30  train_acc: 84.0\n",
      "epoch: 40  train_acc: 85.0\n",
      "epoch: 50  train_acc: 86.0\n",
      "epoch: 60  train_acc: 86.0\n",
      "epoch: 70  train_acc: 87.0\n",
      "epoch: 80  train_acc: 89.0\n",
      "epoch: 90  train_acc: 89.0\n",
      "epoch: 100  train_acc: 89.0\n"
     ]
    }
   ],
   "source": [
    "layer=[2,4,1]\n",
    "\n",
    "#Xavier initialization\n",
    "w1=np.random.uniform(-1,1,size=(layer[1],layer[0]))*np.sqrt(6/(layer[0]+layer[1]))\n",
    "b1=np.zeros(shape=(layer[1],1))\n",
    "w2=np.random.uniform(-1,1,size=(layer[2],layer[1]))*np.sqrt(6/(layer[1]+layer[2]))\n",
    "b2=np.zeros(shape=(layer[2],1))\n",
    "\n",
    "lr = 1\n",
    "train_idx = -1\n",
    "N = len(x)\n",
    "\n",
    "for epochs in range(101):\n",
    "\n",
    "    #forward pass\n",
    "    #layer1\n",
    "    z = np.matmul(w1,x.T)+b1\n",
    "    l1_activation = sigmoid(z)\n",
    "    l1_activation_gradient = dsigmoid(z)\n",
    "    \n",
    "    #layer2\n",
    "    f = np.matmul(w2,l1_activation)+b2\n",
    "    l2_activation = sigmoid(f)\n",
    "    l2_activation_gradient = dsigmoid(f)\n",
    "\n",
    "    #backward pass    \n",
    "    #layer2\n",
    "    l2_delta = (l2_activation - y)*l2_activation_gradient\n",
    "    \n",
    "    #layer1\n",
    "    l1_delta = np.matmul(w2.T,l2_delta)*l1_activation_gradient\n",
    "#     l1_delta = w2.T*l2_delta*l1_activation_gradient\n",
    "     \n",
    "    #update weights & biases\n",
    "    \n",
    "    w1 -= lr*np.matmul(l1_delta,x)/N\n",
    "    w2 -= lr*np.matmul(l2_delta,l1_activation.T)/N\n",
    "\n",
    "    b1 -= lr*l1_delta.sum(axis=1).reshape(-1,1)/N\n",
    "    b2 -= lr*l2_delta.sum(axis=1)/N\n",
    "    ypred_train = np.where(l2_activation>0.5,1.0,0).flatten()\n",
    "    \n",
    "    if(epochs%10==0):  \n",
    "        # convert to classes\n",
    "        ypred_train = np.where(l2_activation>0.5,1.0,0).flatten()\n",
    "        print(\"epoch:\",epochs,\n",
    "              \" train_acc:\",np.round(accuracy_score(ypred_train,y)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How well does our network implementation fair against implementing it using tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 5ms/sample - loss: 0.2986 - accuracy: 0.5100\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 322us/sample - loss: 0.2953 - accuracy: 0.5100\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 175us/sample - loss: 0.2906 - accuracy: 0.5100\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 231us/sample - loss: 0.2887 - accuracy: 0.5100\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 240us/sample - loss: 0.2859 - accuracy: 0.5100\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 115us/sample - loss: 0.2824 - accuracy: 0.5100\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 189us/sample - loss: 0.2803 - accuracy: 0.5100\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 157us/sample - loss: 0.2785 - accuracy: 0.5100\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 213us/sample - loss: 0.2768 - accuracy: 0.5100\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 143us/sample - loss: 0.2747 - accuracy: 0.5100\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 137us/sample - loss: 0.2744 - accuracy: 0.5100\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 195us/sample - loss: 0.2719 - accuracy: 0.5100\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 133us/sample - loss: 0.2690 - accuracy: 0.5100\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 114us/sample - loss: 0.2672 - accuracy: 0.5100\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 149us/sample - loss: 0.2652 - accuracy: 0.5100\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 193us/sample - loss: 0.2630 - accuracy: 0.5100\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 191us/sample - loss: 0.2616 - accuracy: 0.5100\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 111us/sample - loss: 0.2602 - accuracy: 0.5100\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 105us/sample - loss: 0.2585 - accuracy: 0.5100\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 115us/sample - loss: 0.2569 - accuracy: 0.5100\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 142us/sample - loss: 0.2557 - accuracy: 0.5100\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 112us/sample - loss: 0.2541 - accuracy: 0.5100\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 143us/sample - loss: 0.2529 - accuracy: 0.5100\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 96us/sample - loss: 0.2511 - accuracy: 0.5100\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 89us/sample - loss: 0.2501 - accuracy: 0.5100\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 85us/sample - loss: 0.2487 - accuracy: 0.5200\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 192us/sample - loss: 0.2470 - accuracy: 0.5800\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 134us/sample - loss: 0.2456 - accuracy: 0.5300\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 152us/sample - loss: 0.2446 - accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 114us/sample - loss: 0.2431 - accuracy: 0.6900\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 166us/sample - loss: 0.2418 - accuracy: 0.7000\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 142us/sample - loss: 0.2405 - accuracy: 0.7200\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 130us/sample - loss: 0.2392 - accuracy: 0.7400\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 96us/sample - loss: 0.2379 - accuracy: 0.7400\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 133us/sample - loss: 0.2372 - accuracy: 0.7900\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 107us/sample - loss: 0.2365 - accuracy: 0.7500\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 202us/sample - loss: 0.2347 - accuracy: 0.8000\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 121us/sample - loss: 0.2334 - accuracy: 0.7800\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 96us/sample - loss: 0.2323 - accuracy: 0.8000\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 142us/sample - loss: 0.2306 - accuracy: 0.7800\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 92us/sample - loss: 0.2295 - accuracy: 0.8200\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 170us/sample - loss: 0.2282 - accuracy: 0.8000\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 150us/sample - loss: 0.2275 - accuracy: 0.8300\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 167us/sample - loss: 0.2264 - accuracy: 0.8300\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 92us/sample - loss: 0.2249 - accuracy: 0.8200\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 86us/sample - loss: 0.2237 - accuracy: 0.8300\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 135us/sample - loss: 0.2223 - accuracy: 0.8100\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 172us/sample - loss: 0.2210 - accuracy: 0.8300\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 207us/sample - loss: 0.2196 - accuracy: 0.8200\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 125us/sample - loss: 0.2185 - accuracy: 0.8300\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 175us/sample - loss: 0.2173 - accuracy: 0.8300\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 146us/sample - loss: 0.2164 - accuracy: 0.8400\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 170us/sample - loss: 0.2155 - accuracy: 0.8400\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 152us/sample - loss: 0.2142 - accuracy: 0.8200\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 182us/sample - loss: 0.2133 - accuracy: 0.8300\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 174us/sample - loss: 0.2121 - accuracy: 0.8300\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 233us/sample - loss: 0.2108 - accuracy: 0.8400\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 146us/sample - loss: 0.2095 - accuracy: 0.8400\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 165us/sample - loss: 0.2081 - accuracy: 0.8600\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 149us/sample - loss: 0.2067 - accuracy: 0.8400\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 78us/sample - loss: 0.2055 - accuracy: 0.8400\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 117us/sample - loss: 0.2046 - accuracy: 0.8300\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 123us/sample - loss: 0.2032 - accuracy: 0.8200\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 121us/sample - loss: 0.2018 - accuracy: 0.8400\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 125us/sample - loss: 0.2006 - accuracy: 0.8400\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 112us/sample - loss: 0.1994 - accuracy: 0.8400\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 108us/sample - loss: 0.1981 - accuracy: 0.8500\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 127us/sample - loss: 0.1970 - accuracy: 0.8500\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 129us/sample - loss: 0.1956 - accuracy: 0.8500\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 76us/sample - loss: 0.1944 - accuracy: 0.8500\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 71us/sample - loss: 0.1930 - accuracy: 0.8600\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 93us/sample - loss: 0.1921 - accuracy: 0.8400\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 130us/sample - loss: 0.1908 - accuracy: 0.8500\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 115us/sample - loss: 0.1898 - accuracy: 0.8400\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 132us/sample - loss: 0.1887 - accuracy: 0.8400\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 121us/sample - loss: 0.1875 - accuracy: 0.8500\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 112us/sample - loss: 0.1860 - accuracy: 0.8400\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 109us/sample - loss: 0.1848 - accuracy: 0.8400\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 99us/sample - loss: 0.1837 - accuracy: 0.8500\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 121us/sample - loss: 0.1823 - accuracy: 0.8500\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 151us/sample - loss: 0.1812 - accuracy: 0.8500\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 119us/sample - loss: 0.1801 - accuracy: 0.8600\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 185us/sample - loss: 0.1788 - accuracy: 0.8600\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 122us/sample - loss: 0.1775 - accuracy: 0.8500\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 119us/sample - loss: 0.1766 - accuracy: 0.8500\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 119us/sample - loss: 0.1756 - accuracy: 0.8500\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 123us/sample - loss: 0.1747 - accuracy: 0.8500\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 134us/sample - loss: 0.1741 - accuracy: 0.8600\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 117us/sample - loss: 0.1730 - accuracy: 0.8600\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 91us/sample - loss: 0.1715 - accuracy: 0.8500\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 119us/sample - loss: 0.1709 - accuracy: 0.8600\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 118us/sample - loss: 0.1696 - accuracy: 0.8500\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 119us/sample - loss: 0.1683 - accuracy: 0.8600\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 117us/sample - loss: 0.1672 - accuracy: 0.8500\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 87us/sample - loss: 0.1661 - accuracy: 0.8500\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 152us/sample - loss: 0.1649 - accuracy: 0.8500\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 95us/sample - loss: 0.1638 - accuracy: 0.8500\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 97us/sample - loss: 0.1628 - accuracy: 0.8700\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 107us/sample - loss: 0.1618 - accuracy: 0.8500\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 127us/sample - loss: 0.1608 - accuracy: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f69b79b72b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(4,input_shape=(2,),activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(0.1),loss='mean_squared_error',metrics=['accuracy'])\n",
    "model.fit(x,y,epochs=100,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy scores are close in both the implementations, seems like we've done a good job !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next tutorial we will see how easy it is to generalize this approach to implement any n-layer neural network considering multiple neurons in the output layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
